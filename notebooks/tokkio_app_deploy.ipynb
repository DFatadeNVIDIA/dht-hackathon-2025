{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c33a65a-6e5b-4ce1-b2d1-9d4ed76c8932",
   "metadata": {},
   "source": [
    "# Deploying Tokkio\n",
    "\n",
    "In this notebook, we will walk through the steps to deploy the Tokkio workflow on a Bare-metal machine with manual steps. We will use COTURN service to relay media between the Client and Tokkio Back-end. We will work directly on the application instance where the application will be deployed. At a high level the notebook will perform the following:\n",
    "\n",
    "- Install & BootStrap Kubernetes Cluster Using NVIDIA Cloud Native Stack\n",
    "- Setup A TURN Server\n",
    "- Setup Tokkio Foundational Charts:\n",
    "    - Install Local Path Provisioner\n",
    "    - Install Tokkio Observability Helm Chart\n",
    "- Setup Tokkio Application Chart\n",
    "\n",
    "We'll be referencing the official [Tokkio documentation](https://docs.nvidia.com/ace/tokkio/5.0.0-beta/deployment/bare-metal-manual-setup.html#bare-metal-manual-setup) for setting up the Tokkio application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9424f774-94d4-410b-9ddb-f0814ee13458",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "c300bbbc-d7db-4c86-ba45-658c4ba52d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722684aa-ea38-4ec8-9fbd-63d972e3c5da",
   "metadata": {},
   "source": [
    "## Set Secrets\n",
    "\n",
    "Tokkio leverages a number of secrets in order to function properly. At a high-level, the following secrets are needed for the Tokkio deployment:\n",
    "\n",
    "- `OPENAI_API_KEY`: Key to access OpenAI’s models through the API.\n",
    "- `NGC_CLI_API_KEY`: Key to access NGC resources thru command line. Such as Helm charts, Models, Container images etc. Please make sure to generate a **Personal Key** in an NGC org that has access to Tokkio 5.0.\n",
    "- `NVIDIA_API_KEY`: Key to access NGC resources thru command line. Such as Helm charts, Models, Container images etc. This one is specifically used to access NVIDIA NIMs. Please make sure to generate a **Personal Key** in an NGC org that has access to Tokkio 5.0.\n",
    "- `ELEVENLABS_API_KEY`: Key to access ElevenLabs API. ElevenLabs creates highly realistic AI voices for text-to-speech and voice cloning, used in audio content and accessibility.\n",
    "\n",
    "Once you've generated API keys for each service, we can set the values below and reference them in our deployment later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "673535c2-fe11-4e31-be85-10e11ff41c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NGC_CLI_API_KEY\"] = \"<YOUR_NVIDIA_API_KEY>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"<YOUR_NVIDIA_API_KEY>\"\n",
    "os.environ[\"ELEVENLABS_API_KEY\"] = \"<YOUR_ELEVEN_LABS_API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1dedf-9e23-4208-9c0d-1da4add87550",
   "metadata": {},
   "source": [
    "## Configure TURN Server Settings\n",
    "\n",
    "As mentioned in the `deploy_turn_server.ipynb` notebook, in order to interact with our Tokkio Avatar, we'll need to set up a media relay service. We'll be using the COTURN server we configured as the media relay service. Below we provide the following configurations used to create the TURN server:\n",
    "\n",
    "- `TURNSERVER_USERNAME` - The username used to configure the COTURN server\n",
    "- `TURNSERVER_PASSWORD` - The password used to configure the COTRUN server\n",
    "- `TURNSERVER_IP` - The public IP address of the instance the COTURN server is being hosted on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "cf60a6e4-0e02-4c29-8fdc-a097569cb998",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TURNSERVER_USERNAME\"] = \"<YOUR_TURN_SERVER_USERNAME>\"  # Username\n",
    "os.environ[\"TURNSERVER_PASSWORD\"] = \"<YOUR_TURN_SERVER_PASSWORD>\"  # Password\n",
    "os.environ[\"TURNSERVER_IP\"] = \"<YOUR_TURN_SERVER_IP>\" # IP address TURN server is hosted on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed035f3d-135b-4083-b72b-10f4d859d47c",
   "metadata": {},
   "source": [
    "## Install NVIDIA Cloud Native Stack (CNS)\n",
    "\n",
    "NVIDIA Cloud Native Stack (CNS) is an open-source, reference architecture designed to simplify the deployment and management of GPU-accelerated, cloud-native workloads using Kubernetes. It is specifically tailored for AI, data science, and high-performance computing applications that leverage NVIDIA GPUs. Below is a table of the components CNS provides out of the box:\n",
    "\n",
    "| Component                    | Purpose/Functionality                                                                 |\n",
    "|------------------------------|--------------------------------------------------------------------------------------|\n",
    "| **Operating System**         | Ubuntu (22.04/24.04), RHEL                                                           |\n",
    "| **Kubernetes**               | Container orchestration platform                                                     |\n",
    "| **Helm**                     | Kubernetes package manager                                                           |\n",
    "| **Container Runtime**        | Containerd or CRI-O for managing containers                                          |\n",
    "| **NVIDIA GPU Operator**      | Automates GPU driver and toolkit installation, GPU lifecycle management              |\n",
    "| **NVIDIA Network Operator**  | Automates deployment of NVIDIA networking drivers and features                       |\n",
    "| **NVIDIA Container Toolkit** | Enables GPU support in containers                                                    |\n",
    "| **NVIDIA Data Center Driver**| Required GPU drivers for Kubernetes nodes                                            |\n",
    "| **CNI (e.g., Calico)**       | Container networking interface for Kubernetes networking                             |\n",
    "| **Optional Add-ons**         | MicroK8s, storage solutions, load balancers, monitoring tools, KServe for inference  |\n",
    "| **Monitoring Tools**         | GPU metrics export (e.g., DCGM-Exporter), integration with Prometheus                |\n",
    "| **Device Plugins**           | Enable Kubernetes to schedule and manage GPU, RDMA, and SR-IOV resources             |\n",
    "\n",
    "We'll leverage CNS to set up a GPU enabled Kubernetes cluster that can be used to run the Tokkio App. First we'll start by creating a `repos` directory to store the CNS Github repo in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae6bbf4-bb88-4342-bc9f-aacfb44132a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../repos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed08ce-5d28-4a5e-86e4-a65675c27c8e",
   "metadata": {},
   "source": [
    "#### Clone CNS Repo\n",
    "\n",
    "Once we've created the `repos` directory, we can clone the CNS repo. Notice the clone command below; we are cloning a specific tag. Change this according to your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ea9472-8e3a-401b-918d-130c838cc894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '../repos/cloud-native-stack'...\n",
      "Note: switching to '36cefa2ccd9700c299682ede909175e4ef3252f0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 68\n",
      "-rwxr-xr-x 1 ubuntu ubuntu  2249 May  6 15:34 CONTRIBUTING.md\n",
      "-rwxr-xr-x 1 ubuntu ubuntu 11356 May  6 15:34 LICENSE\n",
      "-rwxr-xr-x 1 ubuntu ubuntu  9268 May  6 15:34 README.md\n",
      "-rw-r--r-- 1 ubuntu ubuntu 25260 May  6 15:34 cns.json\n",
      "drwxr-xr-x 3 ubuntu ubuntu  4096 May  6 15:34 install-guides\n",
      "drwxr-xr-x 6 ubuntu ubuntu  4096 May  6 15:34 playbooks\n",
      "drwxr-xr-x 2 ubuntu ubuntu  4096 May  6 15:34 troubleshooting\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# create subdirectory for cloud native stack\n",
    "mkdir -p ../repos/cloud-native-stack\n",
    "\n",
    "# clone NVIDIA cloud native stack repo\n",
    "git clone --branch v24.11.2 --single-branch https://github.com/NVIDIA/cloud-native-stack.git ../repos/cloud-native-stack\n",
    "\n",
    "# view files in cloned repo\n",
    "ls -l ../repos/cloud-native-stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9096aa60-7c7d-4708-a446-8a51f749fd38",
   "metadata": {},
   "source": [
    "#### Configure CNS `hosts` File\n",
    "\n",
    "Once cloned, we'll need to update the `hosts` file under the `../repos/cloud-native-stack/playbooks` directory. This file will contain the IP address of the application instance and the username and password to access it. Normally the contents of the file would follow if you plan on deploying tokkio on a remote instance:\n",
    "\n",
    "```\n",
    "[master]\n",
    "<application-instance-ip> ansible_ssh_user=<username> ansible_ssh_pass=<password> ansible_ssh_common_args='-o StrictHostKeyChecking=no'\n",
    "[nodes]\n",
    "```\n",
    "\n",
    "In our case, we're deploying the Tokkio application on our localhost, on a single master node (our Brev instance), so the content of our file will look like the following below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef02242a-c907-4ef4-ac7e-6436c36b9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../repos/cloud-native-stack/playbooks/hosts <<EOF\n",
    "[master]\n",
    "localhost ansible_connection=local\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99c705-43fa-4693-acdc-686d66d8732e",
   "metadata": {},
   "source": [
    "Verify the Config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4f811d-29fb-4fee-830f-974c8790c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master]\n",
      "localhost ansible_connection=local\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ../repos/cloud-native-stack/playbooks/hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1317f6c-6a87-42d0-8352-19b761696d1a",
   "metadata": {},
   "source": [
    "#### Configure CNS Version and Enable CNS Driver\n",
    "\n",
    "In the cell below, we'll configure our CNS version to 14.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b530510-218d-4b4f-9dc1-7166c41e1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"14.0\" > ../repos/cloud-native-stack/playbooks/cns_version.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7513e5f-d5bf-4629-8f88-df507b1dadfc",
   "metadata": {},
   "source": [
    "Once the CNS version has been configured, we'll need to edit the `cns_values_14.0.yaml` file in the `../repos/cloud-native-stack/playbooks/` directory with the following updated entry:\n",
    "\n",
    "```\n",
    "cns_nvidia_driver: yes\n",
    "```\n",
    "\n",
    "This will install the CNS driver. Once the file has been updated, from the root of the `repos` directory in a seperate terminal, please run the following code block to execute the CNS installation script.\n",
    "\n",
    "```\n",
    "cd cloud-native-stack/playbooks\n",
    "bash setup.sh install\n",
    "```\n",
    "\n",
    "It's a requirement to run the script within the `playbooks` directory to get the install step to complete successfully. This step will take a while to complete; about 5-10 minutes. However if you see the terminal output hanging at the below steup, you may have to kill the process and restart or start a new terminal session and run the `bash setup.sh install` command again. If an install does not complete, running a `bash setup.sh uninstall` can be used to uninstall CNS before reapplying an install.\n",
    "\n",
    "Once the setup completes, you should be able to run the command below to get the status of the GPU Operator pods. If everything is either in a `completed` or `running` state the CNS install step completed successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "194041ad-c284-4824-bbd0-0d6c8414a965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                              READY   STATUS      RESTARTS   AGE\n",
      "gpu-feature-discovery-mmldx                                       1/1     Running     0          2m11s\n",
      "gpu-operator-1746546513-node-feature-discovery-gc-8545dfbblxw6z   1/1     Running     0          2m21s\n",
      "gpu-operator-1746546513-node-feature-discovery-master-7458xlnf2   1/1     Running     0          2m21s\n",
      "gpu-operator-1746546513-node-feature-discovery-worker-lvw9x       1/1     Running     0          2m21s\n",
      "gpu-operator-59bd9fd876-2tx2q                                     1/1     Running     0          2m21s\n",
      "nvidia-cuda-validator-hcht2                                       0/1     Completed   0          108s\n",
      "nvidia-dcgm-exporter-6lkdx                                        1/1     Running     0          2m12s\n",
      "nvidia-device-plugin-daemonset-nctth                              1/1     Running     0          2m12s\n",
      "nvidia-operator-validator-sdln6                                   1/1     Running     0          2m12s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl get pods -n nvidia-gpu-operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f0083-4299-4ab6-85b4-b15a97ebe777",
   "metadata": {},
   "source": [
    "## Setting Up Tokkio Foundational Charts\n",
    "\n",
    "In this section, we will be setting up the Foundational Charts. This will include the local path provisioner and the observability stack. The Rancher Local Path Provisioner automatically creates Kubernetes persistent volumes using local storage on each node, simplifying local storage management for cluster workloads. \n",
    "\n",
    "The observability stack we'll deploy will leverage Prometheus & Grafana. Prometheus is an open-source monitoring and alerting tool that collects and stores metrics as time-series data, commonly used for monitoring cloud-native environments like Kubernetes. Grafana is an open-source data visualization platform that displays metrics from sources like Prometheus using interactive dashboards, charts, and graphs. \n",
    "\n",
    "Let's get started with providing an overrides file for our Local path provisioner deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dfc8c62-8bf3-41d6-a323-cd2d25562ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo 'nodePathMap:\n",
    "  - node: DEFAULT_PATH_FOR_NON_LISTED_NODES\n",
    "    paths:\n",
    "      - /opt/local-path-provisioner\n",
    "storageClass:\n",
    "  name: mdx-local-path\n",
    "'| envsubst > /tmp/local-path-provisioner-values.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08924b63-3236-414e-9a2e-056660410849",
   "metadata": {},
   "source": [
    "#### Fetch Helm Repositories for Local Path Provisioner \n",
    "\n",
    "Once the overrides file has been created, we can fetch the helm repositories with the local path provisioner chart we want to install, make sure you have internet access in order to successfully pull public charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cda87779-4bdd-47fe-8fb2-5fe2898065a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"containeroo\" has been added to your repositories\n",
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"containeroo\" chart repository\n",
      "...Successfully got an update from the \"nvidia\" chart repository\n",
      "Update Complete. ⎈Happy Helming!���\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "helm repo add containeroo https://containeroo.github.io/helm-charts\n",
    "helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde613b-c2a2-4323-bff0-280e025261f5",
   "metadata": {},
   "source": [
    "#### Install The Local Path Provisioner\n",
    "\n",
    "Once the chart has been fetched, we can install the Local Path Provisioner using the `/tmp/local-path-provisioner-values.yml` overrides file we configured earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad953ebe-eeec-4d60-b32e-e4e7f2caafde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"local-path-provisioner\" does not exist. Installing it now.\n",
      "NAME: local-path-provisioner\n",
      "LAST DEPLOYED: Tue May  6 15:51:54 2025\n",
      "NAMESPACE: platform\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "You can create a hostpath-backed persistent volume with a persistent volume claim like this:\n",
      "\n",
      "apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  name: local-path-pvc\n",
      "spec:\n",
      "  accessModes:\n",
      "    - ReadWriteOnce\n",
      "  storageClassName: mdx-local-path\n",
      "  resources:\n",
      "    requests:\n",
      "      storage: 2Gi\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "helm upgrade --install local-path-provisioner containeroo/local-path-provisioner \\\n",
    "  --namespace platform \\\n",
    "  --create-namespace \\\n",
    "  --version 0.0.32 \\\n",
    "  -f /tmp/local-path-provisioner-values.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adb524-d141-4535-bffb-e03b1eb1c8a4",
   "metadata": {},
   "source": [
    "#### Verify Local Path Provisioner\n",
    "\n",
    "Once installed, we can verify the creation of the `mdx-local-path` storage class using the command below. Ensure that this storageclass is using the `cluster.local/local-path-provisioner` provisioner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29303d54-56e3-4ffa-b9f3-251888fa52b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             PROVISIONER                            RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\n",
      "mdx-local-path   cluster.local/local-path-provisioner   Delete          WaitForFirstConsumer   true                   3s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl get storageclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bda1f3-fb39-4f07-be0c-5df4bd3bf6e2",
   "metadata": {},
   "source": [
    "If the output returns as expected, the local path provisioner has been configured! Now we can move on to setting up the observabilty chart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e1c94-a3e1-4349-9e4f-37514c525bf4",
   "metadata": {},
   "source": [
    "#### Fetch Helm Repositories for Foundational Chart Observability Stack\n",
    "\n",
    "As mentioned earlier, the observability stack we'll deploy will leverage Prometheus & Grafana. In order to pull these charts, we'll need access to an NGC API key that can be used to pull resources from NGC. We'll leverage the same secret that was set at the beginning of this notebook to facilitate this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52fa935d-2b75-4bd2-8f7b-a29d8b4e01fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nvidia-ace\" has been added to your repositories\n",
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"containeroo\" chart repository\n",
      "...Successfully got an update from the \"nvidia\" chart repository\n",
      "...Successfully got an update from the \"nvidia-ace\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "helm repo add nvidia-ace https://helm.ngc.nvidia.com/nvidia/ace --username '$oauthtoken' --password $NGC_CLI_API_KEY\n",
    "helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aa68b8-7add-4307-9e8f-93406502ab31",
   "metadata": {},
   "source": [
    "#### Install The Foundational Chart Observability Stack\n",
    "\n",
    "Once the chart has been fetched, we can install the Observability Stack using the it's default helm values, we specify version `0.0.5-beta` to be installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc14d9fc-7516-4806-bf45-a47242dbcf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"obs\" does not exist. Installing it now.\n",
      "NAME: obs\n",
      "LAST DEPLOYED: Tue May  6 15:52:06 2025\n",
      "NAMESPACE: platform\n",
      "STATUS: deployed\n",
      "REVISION: 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "helm upgrade --install obs \\\n",
    "  nvidia-ace/ucf-foundational-chart-observability-stack \\\n",
    "  --version 0.0.5-beta \\\n",
    "  --namespace platform \\\n",
    "  --create-namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a5f19-fb39-4dbe-b01c-79d0da1d873a",
   "metadata": {},
   "source": [
    "#### Verify Observability Stack Is Running\n",
    "\n",
    "Once installed, we can verify that our observabilty pod stack is running using the command below. This will return all the status of the observability stack pods in addition to the local path provisioner pod that was created as part of the install process earlier. If all the pods are in a running state, we can move on to deploying the Tokkio application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddf6d3e6-e676-49c2-a7f2-979f00f0776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                 READY   STATUS    RESTARTS   AGE\n",
      "local-path-provisioner-6df889f859-w7q2c              1/1     Running   0          2m33s\n",
      "obs-grafana-6d7444b987-7n84x                         3/3     Running   0          2m11s\n",
      "obs-kube-prometheus-stack-operator-cd6d489dd-6lbpk   1/1     Running   0          2m11s\n",
      "obs-kube-state-metrics-56f7b5474-9lfms               1/1     Running   0          2m11s\n",
      "obs-loki-0                                           1/1     Running   0          2m11s\n",
      "obs-opentelemetry-collector-75b9675698-b9mqr         1/1     Running   0          2m11s\n",
      "obs-prometheus-node-exporter-dstrg                   1/1     Running   0          2m11s\n",
      "obs-promtail-brrds                                   1/1     Running   0          2m11s\n",
      "obs-tempo-0                                          1/1     Running   0          2m11s\n",
      "prometheus-obs-kube-prometheus-stack-prometheus-0    2/2     Running   0          117s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl get pods -n platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33523c-d862-4f7b-b682-65a9decd8337",
   "metadata": {},
   "source": [
    "## Tokkio Application Chart Setup\n",
    "\n",
    "In this section, we will be setting up the Tokkio Helm Chart. This will include the creation of the application namespace and the secrets required for the application. Everything that has been done up until this point has been a prerequisite step to enable to the deployment of the Tokkio application! Firstly, we'll get started with creating a namespace for the application - this will be used to manage all the resources tied to the deploment of the Tokkio app helm chart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d329fa0-f93b-4658-b646-819be1814d53",
   "metadata": {},
   "source": [
    "#### Create Tokkio App Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "90a44b60-8da6-443c-b171-480948028cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/app created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl create namespace app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722ee95-2fde-469c-be7e-e4ee5bede0d2",
   "metadata": {},
   "source": [
    "#### Verify Creation Of Tokkio App Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "41ff63c2-36e5-4538-b673-c34106f31c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME   STATUS   AGE\n",
      "app    Active   1s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl get ns app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e2ea3-9700-436d-83b6-318c518e8814",
   "metadata": {},
   "source": [
    "#### Create Tokkio Kubernetes Secrets\n",
    "\n",
    "We previously created environment variables representing the secrets our Tokkio application will leverage at the beginning of this notebook. Kubernetes Secrets securely store sensitive data (like passwords or API keys) and make it available to applications running in Pods, helping protect sensitive information within your cluster. We'll create a kubernetes secret for each variable, as well as a `docker-registry` secret type that can be used to pull images and resources from NGC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "0b7813ca-ba0a-49f1-8173-c32770c64e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/ngc-api-key-secret created\n",
      "secret/openai-key-secret created\n",
      "secret/nvidia-api-key-secret created\n",
      "secret/ngc-docker-reg-secret created\n",
      "secret/elevenlabs-api-key-secret created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# NGC API Key Secret\n",
    "kubectl create secret generic ngc-api-key-secret \\\n",
    "  --from-literal=NGC_CLI_API_KEY=\"${NGC_CLI_API_KEY}\" -n app\n",
    "\n",
    "# OpenAI Key Secret\n",
    "kubectl create secret generic openai-key-secret \\\n",
    "  --from-literal=OPENAI_API_KEY=\"${OPENAI_API_KEY}\" -n app\n",
    "\n",
    "# NVIDIA API Key Secret\n",
    "kubectl create secret generic nvidia-api-key-secret \\\n",
    "  --from-literal=NVIDIA_API_KEY=\"${NVIDIA_API_KEY}\" -n app\n",
    "\n",
    "# NGC Docker Registry Secret\n",
    "kubectl create secret docker-registry ngc-docker-reg-secret \\\n",
    "  --docker-server=nvcr.io \\\n",
    "  --docker-username='$oauthtoken' \\\n",
    "  --docker-password=\"${NGC_CLI_API_KEY}\" -n app\n",
    "\n",
    "# ElevenLabs API Key Secret\n",
    "kubectl create secret generic elevenlabs-api-key-secret \\\n",
    "  --from-literal=ELEVENLABS_API_KEY=\"${ELEVENLABS_API_KEY}\" -n app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec4670-4f1b-4399-8b82-ca5b6dda8349",
   "metadata": {},
   "source": [
    "#### Verify Creation Of Tokkio App Secrets\n",
    "\n",
    "We can use the command below to verify creation of all of the secrets our Tokkio Application will leverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "326afe3c-92ea-4310-9a05-3e6fc7573c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        TYPE                             DATA   AGE\n",
      "elevenlabs-api-key-secret   Opaque                           1      2s\n",
      "ngc-api-key-secret          Opaque                           1      3s\n",
      "ngc-docker-reg-secret       kubernetes.io/dockerconfigjson   1      2s\n",
      "nvidia-api-key-secret       Opaque                           1      2s\n",
      "openai-key-secret           Opaque                           1      3s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl get secrets -n app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf45fbf-78ee-422b-ae5a-f4fa58e6e254",
   "metadata": {},
   "source": [
    "#### Generate Tokkio App Overrides File\n",
    "\n",
    "Here we will generate the override values file. This file will contain the configuration for the Tokkio Helm Chart, and will be used in helm install command. We'll configure our `TURN_SERVER_IP` using the public IP address we obtained earlier. We'll also configure the application to use the COTURN server we configured earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b280f763-d965-4286-8a05-f8f66c04cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOF | envsubst > /tmp/ucf-tokkio-audio-video-app.yml\n",
    "tokkio-app:\n",
    "  vms:\n",
    "    configs:\n",
    "      vst_config.json:\n",
    "        network:\n",
    "          twilio_account_sid: \"\"\n",
    "          twilio_auth_token: \"\"\n",
    "          use_twilio_stun_turn: false\n",
    "          static_turnurl_list:\n",
    "          - \"${TURNSERVER_USERNAME}:${TURNSERVER_PASSWORD}@${TURNSERVER_IP}:3478\"\n",
    "          use_reverse_proxy: false\n",
    "\n",
    "  ia-unreal-renderer-microservice:\n",
    "    configs:\n",
    "      IAUEMS_SIGNALLING_SERVER_PEER_CONNECTION_OPTIONS: |\n",
    "        {\n",
    "          \"iceServers\": [\n",
    "            {\n",
    "              \"urls\": [\"turn:${TURNSERVER_IP}:3478\"],\n",
    "              \"username\": \"${TURNSERVER_USERNAME}\",\n",
    "              \"credential\": \"${TURNSERVER_PASSWORD}\"\n",
    "            }\n",
    "          ],\n",
    "          \"iceTransportPolicy\": \"relay\"\n",
    "        }\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9295e-cfb7-4a95-92f9-9503affbcdbe",
   "metadata": {},
   "source": [
    "#### View Generated Overrides File\n",
    "\n",
    "Before installing the Tokkio application, we can verify the contents of the override file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "3b588005-67f9-4f35-bab2-b10e5f9540f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokkio-app:\n",
      "  vms:\n",
      "    configs:\n",
      "      vst_config.json:\n",
      "        network:\n",
      "          twilio_account_sid: \"\"\n",
      "          twilio_auth_token: \"\"\n",
      "          use_twilio_stun_turn: false\n",
      "          static_turnurl_list:\n",
      "          - \"<YOUR_TURN_SERVER_USERNAME>:<YOUR_TURN_SERVER_PASSWORD>@<YOUR_TURN_SERVER_IP>:3478\"\n",
      "          use_reverse_proxy: false\n",
      "\n",
      "  ia-unreal-renderer-microservice:\n",
      "    configs:\n",
      "      IAUEMS_SIGNALLING_SERVER_PEER_CONNECTION_OPTIONS: |\n",
      "        {\n",
      "          \"iceServers\": [\n",
      "            {\n",
      "              \"urls\": [\"turn:<YOUR_TURN_SERVER_IP>:3478\"],\n",
      "              \"username\": \"<YOUR_TURN_SERVER_USERNAME>\",\n",
      "              \"credential\": \"<YOUR_TURN_SERVER_PASSWORD>\"\n",
      "            }\n",
      "          ],\n",
      "          \"iceTransportPolicy\": \"relay\"\n",
      "        }\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat /tmp/ucf-tokkio-audio-video-app.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b013b4f-4e5b-42d8-8586-5c039f9f9418",
   "metadata": {},
   "source": [
    "#### Install Tokkio App Helm Chart\n",
    "\n",
    "Now, we will install the Tokkio helm chart using the override values file and the NGC CLI API key. We're pulling and installing the chart that is configured to use 1 tokkio stream in the `tokkio-1stream-with-ui-5.0.0-beta.tgz`. If we wanted to leverage a chart that uses 3 streams for example, we can use the `tokkio-3stream-with-ui-5.0.0-beta.tgz` chart. Please note that this chart would require a setup using 4x GPUs without GPU time-slicing enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "72758479-2b03-4621-84db-382b6ae42309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"tokkio-app\" does not exist. Installing it now.\n",
      "NAME: tokkio-app\n",
      "LAST DEPLOYED: Tue May  6 18:27:11 2025\n",
      "NAMESPACE: app\n",
      "STATUS: deployed\n",
      "REVISION: 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "helm upgrade \\\n",
    "  --install \\\n",
    "  --reset-values \\\n",
    "  --create-namespace \\\n",
    "  --namespace app \\\n",
    "  --username '$oauthtoken' \\\n",
    "  --password \"${NGC_CLI_API_KEY}\" \\\n",
    "  --values /tmp/ucf-tokkio-audio-video-app.yml \\\n",
    "  tokkio-app \\\n",
    "  https://helm.ngc.nvidia.com/nvidia/ace/charts/tokkio-1stream-with-ui-5.0.0-beta.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e66167-49b3-49a0-a5be-051fbb44c7c9",
   "metadata": {},
   "source": [
    "#### Verify Tokkio App Pods Are Running\n",
    "\n",
    "Once installed, we can verify the Tokkio app is up and running using the command below. Please note it could take anywhere from 30-45 minutes for all the pods to enter a running state. Once running, we can access the app from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "835789bd-2950-4cc3-b586-e02a844372f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                      READY   STATUS    RESTARTS   AGE\n",
      "a2f-a2f-deployment-7c48f84597-j5qs8                       1/1     Running   0          10m\n",
      "ace-configurator-deployment-8649bffb8f-r4h5n              1/1     Running   0          10m\n",
      "ace-controller-ace-controller-deployment-0                1/1     Running   0          10m\n",
      "ace-controller-sdr-envoy-sdr-deployment-f58d99587-nk256   4/4     Running   0          10m\n",
      "anim-graph-sdr-envoy-sdr-deployment-86744bb9-mtzhw        4/4     Running   0          9m59s\n",
      "ia-animation-graph-microservice-deployment-0              1/1     Running   0          10m\n",
      "ia-unreal-renderer-microservice-deployment-0              3/3     Running   0          10m\n",
      "redis-redis-755b568f59-hthv5                              1/1     Running   0          10m\n",
      "redis-timeseries-redis-timeseries-5dc5ccd4cf-qm4cz        1/1     Running   0          10m\n",
      "riva-speech-57b77f9466-xmcxw                              1/1     Running   0          10m\n",
      "tokkio-ingress-mgr-deployment-86879666b4-j4xh6            2/2     Running   0          10m\n",
      "tokkio-ui-deployment-6985b5d74-4jmlt                      1/1     Running   0          10m\n",
      "triton0-97cbb4744-x9twp                                   1/1     Running   0          10m\n",
      "ue-renderer-sdr-envoy-sdr-deployment-5f77dffddb-vd4wd     4/4     Running   0          9m59s\n",
      "vms-vms-d54bd4698-f27q2                                   1/1     Running   0          10m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl get pods -n app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90ba6f-a66f-4646-af6e-e9c8b089844d",
   "metadata": {},
   "source": [
    "Once all the pods are in a running state, we can access the Tokkio application via the Tokkio UI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24bf00-71d7-4363-9734-186950ae934c",
   "metadata": {},
   "source": [
    "## Accessing The Tokkio UI\n",
    "\n",
    "Before attemtping to access the UI, please make sure the following ports are exposed either publically, or on networks you plan on accessing the application from:\n",
    "\n",
    "- Port `30111`\n",
    "- Port `30180`\n",
    "- Port `30443`\n",
    "- Port `32300`\n",
    "\n",
    "These ports need to be open to ensure the respective Tokkio services can communicate with each other. Once all these ports have been opened up, we're good to go! To launch the UI in your browser, simply navigate to the following endpoint `https://<app-instance-ip>:30111`\n",
    "\n",
    "Once accessed, click the “Advanced” when prompted about the unsigned certificate. Review the security message and click “Proceed to…” to continue. Allow audio permissions when prompted by the browser, as you will need an active microphone to interact with the avatar. If the avatar comes up you're good to go! If you run into issues accessing the avatar, please refer to the [Tokkio deployment troubleshooting page](https://docs.nvidia.com/ace/tokkio/5.0.0-beta/deployment/troubleshooting.html)\n",
    "\n",
    "**Note:** The unsigned certificate warning is expected for development environments. Production deployments should use proper SSL certificates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a230b6a-f32e-4f15-8a14-12486e566586",
   "metadata": {},
   "source": [
    "## Accessing Observability\n",
    "\n",
    "Tokkio observability stack helps users collect logs, traces, and metrics across the entire system and provides a single pane of glass for data visualization.\n",
    "\n",
    "The observability stack is an open-source observability stack designed for log aggregation (Loki) and data visualization (Grafana). and distributed tracing (Tempo). It provides a cost-effective and scalable way to monitor, debug, and analyze modern cloud-native applications. Each component serves a specific purpose in the observability pipeline. For more on the observability stack and it's components, refer to the [Tokkio Observabilty](https://docs.nvidia.com/ace/tokkio/5.0.0-beta/observability/observability.html) documentation page.\n",
    "\n",
    "You can access the Grafana service from the endpoint: `http://<application-instance-ip>:32300/`. By default the following credentials are configured:\n",
    "\n",
    "- Username: `admin`\n",
    "- Password: `admin`\n",
    "\n",
    "Once you login with these credentials, you have the option to set a new password. For usage and how you can explore traces, metrics and other data points, please refer to the [Tokkio Observabilty Usage](https://docs.nvidia.com/ace/tokkio/5.0.0-beta/observability/observability-usage.html) documentation page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93948b3a-c4fe-46f8-9ce0-947a9a7009c5",
   "metadata": {},
   "source": [
    "## Accessing The ACE Configurator:\n",
    "\n",
    "The ACE Configurator based developer workflow is designed to enable seamless debugging and streamlined iterative development for Tokkio Digital Human Agents. For more details regarding the ACE Configurator, please refer to the [ACE Configurator](https://docs.nvidia.com/ace/ace-configurator/1.0/index.html#ace-configurator-overview)\n",
    "\n",
    "The workflow satisfies the following use cases:\n",
    "\n",
    "- Modify the agent pipeline during live deployment and instantly view the updated changes.\n",
    "- Changes remain persistent even after pod reboot.\n",
    "- No need to rebuild or redeploy the app for changes supported by the ACE Configurator.\n",
    "\n",
    "We'll cover the ACE Configurator more extensively in the next notebook `customizing_tokkio.ipynb`. This notebook will cover how to get started with customizations by updating the RAG endpoint used for the current Tokkio deployment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
