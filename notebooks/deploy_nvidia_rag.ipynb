{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8536e97-dd3b-47c8-acf3-8b595b536ac0",
   "metadata": {},
   "source": [
    "# Deploying The NVIDIA RAG Blueprint\n",
    "\n",
    "In this notebook, we will deploy a Retrieval-Augmented Generation (RAG) powered AI Chatbot. This system enhances traditional large language models by incorporating external knowledge, allowing the model to provide more accurate and contextually relevant responses. The system works in two key stages:\n",
    "\n",
    "**Data Ingestion Pipeline**\n",
    "- Ingests and processes enterprise data: Ingests and processes user documents. Processing includes detecting and extracting graphic elements such as tables, charts, infographics.\n",
    "- Creates embeddings: Converts text into vector representations that capture semantic meaning\n",
    "- Builds vector database: Stores these embeddings in a searchable database for efficient retrieval\n",
    "\n",
    "**Query / Response Pipeline**\n",
    "- Embeds user queries: Converts user questions into vector embeddings\n",
    "- Retrieves relevant context: Finds semantically similar documents in the vector database\n",
    "- Reranks results: Prioritizes the most relevant information\n",
    "- Generates responses: Uses an LLM to craft comprehensive responses based on retrieved context\n",
    "\n",
    "Both data and queries are encoded as vectors through an embedding process, enabling efficient similarity search based on semantic meaning rather than simple keyword matching. We'll be leveraging the [NVIDIA RAG Blueprint](https://build.nvidia.com/nvidia/build-an-enterprise-rag-pipeline) to setup the RAG service, please refer to the official [GitHub page](https://github.com/NVIDIA-AI-Blueprints/rag/tree/main?tab=readme-ov-file) here as the service will be built off of this code repository. You can also refer to [this list](https://github.com/NVIDIA-AI-Blueprints/rag/tree/main?tab=readme-ov-file#software-components) for the models leveraged in this pipeline.\n",
    "\n",
    "As an additional step, we'll be leveraging models hosted on [build.nvidia.com.](https://build.nvidia.com/) By default, the microservices we deploy, expect to leverage locally hosted NVIDIA NIMs. To simplify this playbook and to ensure users are able to run Tokkio + RAG on the same instance, we'll leverage models hosted on NVIDIA Foundation Endpoints through [build.nvidia.com.](https://build.nvidia.com/) The ONLY service that will be deployed as part of the pipeline that leverages GPU resources will be Milvus - A GPU accelerated vector DB that will be leveraged to store embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c543f-3302-41b4-936d-1aa0fbdbee1b",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c6c9cc-87bb-47ee-a4d6-9bf1ae95c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfaab98-05eb-43da-a255-5281a391c536",
   "metadata": {},
   "source": [
    "### Set Secrets\n",
    "\n",
    "An NVIDIA API Key will be needed in order to access resources from NGC. It will also be leveraged to interact with the models deployed on [build.nvidia.com.](https://build.nvidia.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5700288-4f4b-4010-8eee-d847e41af3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NVIDIA_API_KEY\"] = \"<YOUR_NVIDIA_API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994084a4-5a4a-41b8-bd52-b607a55c6881",
   "metadata": {},
   "source": [
    "### Login To `nvcr.io` Docker Registry\n",
    "\n",
    "We need to login to the NGC registry in order to be able to pull the container images that will be deployed later. We can login to the `nvcr.io` registry using the following code block below. Make sure you have a valid NGC key or login will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1605214f-1baf-4c66-a0a5-2ebaae78ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING! Your credentials are stored unencrypted in '/home/ubuntu/.docker/config.json'.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/go/credential-store/\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"${NVIDIA_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aafc70-f511-44aa-8134-56165264fe7b",
   "metadata": {},
   "source": [
    "### Clone NVIDIA AI RAG BluePrint Repository\n",
    "\n",
    "Once logged in, we can clone the NVIDIA AI RAG BluePrint code repository and store it in the `repos` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7eec9e-684e-44c5-9b5f-26a86dc8c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# clone NVIDIA Generative AI Examples repo\n",
    "git clone https://github.com/NVIDIA-AI-Blueprints/rag.git ../repos/nvidia_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7985a-2fef-4b1a-8fb5-de6dc0d603f7",
   "metadata": {},
   "source": [
    "### Deploying Milvus\n",
    "\n",
    "Milvus is an open-source vector database designed specifically for storing, indexing, and managing large-scale vector data, such as embeddings generated by deep learning models. \n",
    "\n",
    "It excels at efficient similarity search and is highly scalable, supporting billions of vectors and running across environments from laptops to large distributed systems. We'll be deploying this service initially, as the other services that will be deployed will rely on Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56b638-c9bf-4bdb-9b78-fd333ea26c34",
   "metadata": {},
   "source": [
    "#### Pull the Containers Needed For The Milvus Service\n",
    "\n",
    "As a prerequisite, we'll need to pull the containers associated with Milvus before deploying the containers. The code block below will facilitate this process. Once the relevant images are pulled, we can move on to deploying the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c76beb9e-dce0-4a1d-96d8-f33ef217deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " milvus Pulling \n",
      " minio Pulling \n",
      " etcd Pulling \n",
      " minio Pulled \n",
      " milvus Pulled \n",
      " etcd Pulled \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker compose -f ../repos/nvidia_rag/deploy/compose/vectordb.yaml pull --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f889a-fead-4241-a85d-df2ede35d53e",
   "metadata": {},
   "source": [
    "#### Start The Containers For The Milvus Service\n",
    "\n",
    "After successfully pulling the required Milvus-related Docker containers (milvus, etcd, minio), the next step is to start these containers to launch the Milvus service. With the necessary images now available locally, you can proceed to deploy and run the service using the appropriate Docker Compose command. This will bring up the Milvus, etcd, and minio containers, enabling you to use the Milvus vector database service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500d2c63-80a0-475d-af2c-aceddc9d1c92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network nvidia-rag  Creating\n",
      " Network nvidia-rag  Created\n",
      " Container milvus-minio  Creating\n",
      " Container milvus-etcd  Creating\n",
      " Container milvus-etcd  Created\n",
      " Container milvus-minio  Created\n",
      " Container milvus-standalone  Creating\n",
      " Container milvus-standalone  Created\n",
      " Container milvus-minio  Starting\n",
      " Container milvus-etcd  Starting\n",
      " Container milvus-etcd  Started\n",
      " Container milvus-minio  Started\n",
      " Container milvus-standalone  Starting\n",
      " Container milvus-standalone  Started\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker compose -f ../repos/nvidia_rag/deploy/compose/vectordb.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f083ba-9fe8-46bc-96bd-4cc2c7b1444f",
   "metadata": {},
   "source": [
    "#### Check Status Of Milvus Containers\n",
    "\n",
    "After starting the Milvus service containers, verify that all required containers (milvus-standalone, milvus-etcd, milvus-minio) are running. Use the following command to list the active containers and their statuses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef67a221-3f51-4657-8f9b-d4db4c0dfe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   NAMES               STATUS\n",
      "3e1ac2226efd   milvus-standalone   Up 2 seconds\n",
      "1cc93d91a4f3   milvus-minio        Up 2 seconds (health: starting)\n",
      "9f1091c4de6a   milvus-etcd         Up 2 seconds (health: starting)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker ps --format \"table {{.ID}}\\t{{.Names}}\\t{{.Status}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452d8a5-8280-4ce5-aab4-3430cc944142",
   "metadata": {},
   "source": [
    "If all the services are healthy or running as expected, we can proceed with deploying the ingestor service!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d9c0e-585d-475a-9394-99d55fd18fe9",
   "metadata": {},
   "source": [
    "### Deploying The Ingestor Service\n",
    "\n",
    "[NVIDIA-Ingest (NV-Ingest)](https://github.com/NVIDIA/nv-ingest/tree/main) is leveraged for ingestion of files. NVIDIA-Ingest is a scalable, performance-oriented document content and metadata extraction microservice. Including support for parsing PDFs, Word and PowerPoint documents, it uses specialized NVIDIA NIM microservices to find, contextualize, and extract text, tables, charts and images for use in downstream generative applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baabe18-4294-4bd8-9d18-cacd390aeacc",
   "metadata": {},
   "source": [
    "#### Pull The Containers Needed The Ingestor Microservice\n",
    "\n",
    "As a prerequisite, we'll need to pull the containers associated with the ingestor pipeline before deploying the containers. The code block below will facilitate this process. Once the relevant images are pulled, we can move on to configuring parameters that will be used to customize deployment of the ingestor service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a595a99-2d1e-461a-bcf3-15ab15a264ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ingestor-server Pulling \n",
      " nv-ingest-ms-runtime Pulling \n",
      " redis Pulling \n",
      " redis Pulled \n",
      " ingestor-server Pulled \n",
      " nv-ingest-ms-runtime Pulled \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker compose -f ../repos/nvidia_rag/deploy/compose/docker-compose-ingestor-server.yaml pull --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d77c8-d0de-4135-b5af-1b98b588f4fc",
   "metadata": {},
   "source": [
    "#### Configure Parameters For The Ingestor Microservice\n",
    "\n",
    "By default, the Ingestor Microservice, expects to leverage locally hosted NVIDIA NIMs. To simplify this playbook and to ensure users are able to run Tokkio + RAG on the same instance, we'll leverage models hosted on NVIDIA Foundation Endpoints through [build.nvidia.com.](https://build.nvidia.com/)\n",
    "\n",
    "We'll configure the service to use the default LLM & Retriever NIMs enabled in the default RAG blueprint. We'll also set the corresponding Server URL's to point to the corresponding models hosted on build. Please note that changing the model names will effectively change which models are invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a1fb14-b117-4c76-a9eb-2fdab91d1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever NIMs parameters\n",
    "os.environ[\"APP_EMBEDDINGS_MODELNAME\"] = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "os.environ[\"APP_EMBEDDINGS_SERVERURL\"] = \"https://integrate.api.nvidia.com/v1\"\n",
    "\n",
    "# NV-Ingest NIMs endpoints\n",
    "os.environ[\"PADDLE_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/baidu/paddleocr\"\n",
    "os.environ[\"YOLOX_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\"\n",
    "os.environ[\"VLM_CAPTION_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions\"\n",
    "os.environ[\"NEMORETRIEVER_PARSE_HTTP_ENDPOINT\"] = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "\n",
    "# NV-Ingest NIMs protocol must be set to http\n",
    "os.environ[\"PADDLE_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"NEMORETRIEVER_PARSE_INFER_PROTOCOL\"] = \"http\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a572c-2549-40b6-a6da-89bf85bc2897",
   "metadata": {},
   "source": [
    "#### Start The Containers For The Ingestor Microservice\n",
    "\n",
    "With the parameters for this deployment configured, we can start the service using the `docker compose` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "669ec30a-7645-4823-990b-28960c7a35fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=\"2025-05-07T19:33:38Z\" level=warning msg=\"Found orphan containers ([milvus-standalone milvus-minio milvus-etcd]) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\"\n",
      " Container compose-nv-ingest-ms-runtime-1  Creating\n",
      " Container ingestor-server  Creating\n",
      " Container compose-redis-1  Creating\n",
      " Container compose-redis-1  Created\n",
      " Container ingestor-server  Created\n",
      " Container compose-nv-ingest-ms-runtime-1  Created\n",
      " Container compose-nv-ingest-ms-runtime-1  Starting\n",
      " Container compose-redis-1  Starting\n",
      " Container ingestor-server  Starting\n",
      " Container compose-redis-1  Started\n",
      " Container ingestor-server  Started\n",
      " Container compose-nv-ingest-ms-runtime-1  Started\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker compose -f ../repos/nvidia_rag/deploy/compose/docker-compose-ingestor-server.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ef4d0-72df-456c-97b3-8f15699efa00",
   "metadata": {},
   "source": [
    "#### Verify Status Of Ingestor Containers\n",
    "\n",
    "Once the containers have been started, we can verify the status below. Each service should either have a `healthy` status or a non-error status next to the appropriate service name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b244320-cdb5-45a9-9bda-cf34d53d57a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   NAMES                            STATUS\n",
      "ac450966da38   compose-nv-ingest-ms-runtime-1   Up 26 seconds (healthy)\n",
      "ae83680b62d2   ingestor-server                  Up 26 seconds\n",
      "7dd3f3e440f7   compose-redis-1                  Up 26 seconds\n",
      "3e1ac2226efd   milvus-standalone                Up 37 seconds\n",
      "1cc93d91a4f3   milvus-minio                     Up 38 seconds (healthy)\n",
      "9f1091c4de6a   milvus-etcd                      Up 38 seconds (healthy)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker ps --format \"table {{.ID}}\\t{{.Names}}\\t{{.Status}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a2e39-0670-4012-b9eb-e456147e68ec",
   "metadata": {},
   "source": [
    "#### Verify Application Startup Is Successful\n",
    "\n",
    "It's helpful to verify if the application started up as expected, even with the presence of the `healthy` status on the `compose-nv-ingest-ms-runtime-1` service. We can view the status of the service by checking the logs. If we see the following output towards the end without any error logs, the Ingestor Server is operating as expected:\n",
    "\n",
    "```\n",
    "====Building Segment Complete!====\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cc2f2f5-215f-42b7-8afa-d7f17c204b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Uvicorn running on http://0.0.0.0:7670 (Press CTRL+C to quit)\n",
      "INFO:     Started parent process [35]\n",
      "INFO:     Started server process [66]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [44]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [38]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [48]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [67]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [51]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [65]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Started server process [55]\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [53]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [68]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [58]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [49]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [69]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [56]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [61]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [41]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [60]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [50]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [45]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [62]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [39]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [43]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [54]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [47]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [52]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [42]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [46]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [40]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [63]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [64]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [57]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [59]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "2025-05-07 19:33:56,799 - INFO - Starting pipeline setup\n",
      "2025-05-07 19:33:56,800 - INFO - YOLOX_GRPC_ENDPOINT: page-elements:8001\n",
      "2025-05-07 19:33:56,800 - INFO - YOLOX_HTTP_ENDPOINT: https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\n",
      "2025-05-07 19:33:56,800 - INFO - YOLOX_INFER_PROTOCOL: http\n",
      "2025-05-07 19:33:56,800 - INFO - NEMORETRIEVER_PARSE_GRPC_ENDPOINT: \n",
      "2025-05-07 19:33:56,800 - INFO - NEMORETRIEVER_PARSE_HTTP_ENDPOINT: http://nemoretriever-parse:8000/v1/chat/completions\n",
      "2025-05-07 19:33:56,800 - INFO - NEMORETRIEVER_PARSE_INFER_PROTOCOL: http\n",
      "2025-05-07 19:33:56,883 - INFO - YOLOX_GRPC_ENDPOINT: page-elements:8001\n",
      "2025-05-07 19:33:56,884 - INFO - YOLOX_HTTP_ENDPOINT: https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\n",
      "2025-05-07 19:33:56,884 - INFO - YOLOX_INFER_PROTOCOL: http\n",
      "2025-05-07 19:33:56,885 - INFO - YOLOX_GRPC_ENDPOINT: page-elements:8001\n",
      "2025-05-07 19:33:56,885 - INFO - YOLOX_HTTP_ENDPOINT: https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\n",
      "2025-05-07 19:33:56,885 - INFO - YOLOX_INFER_PROTOCOL: http\n",
      "2025-05-07 19:33:56,885 - INFO - YOLOX_GRPC_ENDPOINT: page-elements:8001\n",
      "2025-05-07 19:33:56,885 - INFO - YOLOX_HTTP_ENDPOINT: https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\n",
      "2025-05-07 19:33:56,885 - INFO - YOLOX_INFER_PROTOCOL: http\n",
      "2025-05-07 19:33:56,886 - INFO - AUDIO_GRPC_TRITON: audio:50051\n",
      "2025-05-07 19:33:56,886 - INFO - AUDIO_HTTP_TRITON: \n",
      "2025-05-07 19:33:56,886 - INFO - AUDIO_INFER_PROTOCOL: grpc\n",
      "2025-05-07 19:33:56,887 - INFO - YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT: table-structure:8001\n",
      "2025-05-07 19:33:56,887 - INFO - YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT: https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\n",
      "2025-05-07 19:33:56,887 - INFO - YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL: http\n",
      "2025-05-07 19:33:56,887 - INFO - PADDLE_GRPC_ENDPOINT: paddle:8001\n",
      "2025-05-07 19:33:56,887 - INFO - PADDLE_HTTP_ENDPOINT: https://ai.api.nvidia.com/v1/cv/baidu/paddleocr\n",
      "2025-05-07 19:33:56,887 - INFO - PADDLE_INFER_PROTOCOL: http\n",
      "2025-05-07 19:33:56,887 - INFO - YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT: graphic-elements:8001\n",
      "2025-05-07 19:33:56,887 - INFO - YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT: https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\n",
      "2025-05-07 19:33:56,887 - INFO - YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL: http\n",
      "2025-05-07 19:33:56,887 - INFO - PADDLE_GRPC_ENDPOINT: paddle:8001\n",
      "2025-05-07 19:33:56,887 - INFO - PADDLE_HTTP_ENDPOINT: https://ai.api.nvidia.com/v1/cv/baidu/paddleocr\n",
      "2025-05-07 19:33:56,887 - INFO - PADDLE_INFER_PROTOCOL: http\n",
      "2025-05-07 19:33:56,888 - INFO - PADDLE_GRPC_ENDPOINT: paddle:8001\n",
      "2025-05-07 19:33:56,888 - INFO - PADDLE_HTTP_ENDPOINT: https://ai.api.nvidia.com/v1/cv/baidu/paddleocr\n",
      "2025-05-07 19:33:56,888 - INFO - PADDLE_INFER_PROTOCOL: http\n",
      "2025-05-07 19:33:56,889 - INFO - Pipeline setup completed in 0.09 seconds\n",
      "2025-05-07 19:33:56,890 - INFO - Running pipeline\n",
      "W20250507 19:33:56.954236 138551796569920 topology.cpp:96] skipping device: NVIDIA L40S with pcie: 00000000:3A:00.0; errmsg=invalid device ordinal\n",
      "W20250507 19:33:56.954596 138551796569920 topology.cpp:96] skipping device: NVIDIA L40S with pcie: 00000000:3C:00.0; errmsg=invalid device ordinal\n",
      "W20250507 19:33:56.954668 138551796569920 topology.cpp:96] skipping device: NVIDIA L40S with pcie: 00000000:3E:00.0; errmsg=invalid device ordinal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded and validated: audio_extractor_schema=AudioExtractorSchema(max_queue_size=1, n_workers=16, raise_on_failure=False, audio_extraction_config=None) chart_extractor_module=ChartExtractorSchema(max_queue_size=1, n_workers=2, raise_on_failure=False, stage_config=None) text_splitter_module=TextSplitterSchema(tokenizer=None, chunk_size=1024, chunk_overlap=150, raise_on_failure=False) embedding_storage_module=EmbeddingStorageModuleSchema(raise_on_failure=False) embed_extractions_module=EmbedExtractionsSchema(api_key='api_key', batch_size=8192, embedding_model='nvidia/nv-embedqa-e5-v5', embedding_nim_endpoint='http://embedding:8000/v1', encoding_format='float', httpx_log_level=<LogLevel.WARNING: 'WARNING'>, input_type='passage', raise_on_failure=False, truncate='END') image_caption_extraction_module=ImageCaptionExtractionSchema(api_key='api_key', endpoint_url='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', prompt='Caption the content of this image:', model_name='meta/llama-3.2-11b-vision-instruct', raise_on_failure=False) image_dedup_module=ImageDedupSchema(raise_on_failure=False, cpu_only=False) image_filter_module=ImageFilterSchema(raise_on_failure=False, cpu_only=False) image_storage_module=ImageStorageModuleSchema(structured=True, images=True, raise_on_failure=False) infographic_extractor_module=InfographicExtractorSchema(max_queue_size=1, n_workers=2, raise_on_failure=False, stage_config=None) job_counter_module=JobCounterSchema(name='job_counter', raise_on_failure=False) metadata_injection_module=MetadataInjectorSchema(raise_on_failure=False) otel_meter_module=OpenTelemetryMeterSchema(broker_client=MessageBrokerClientSchema(host='redis', port=6379, client_type='redis', broker_params={}, connection_timeout=300, max_backoff=300, max_retries=0), otel_endpoint='localhost:4317', raise_on_failure=False) otel_tracer_module=OpenTelemetryTracerSchema(otel_endpoint='localhost:4317', raise_on_failure=False) pdf_extractor_module=PDFExtractorSchema(max_queue_size=1, n_workers=16, raise_on_failure=False, pdfium_config=None, nemoretriever_parse_config=None) pptx_extractor_module=PPTXExtractorSchema(max_queue_size=1, n_workers=16, raise_on_failure=False, pptx_extraction_config=None) redis_task_sink=MessageBrokerTaskSinkSchema(broker_client=MessageBrokerClientSchema(host='redis', port=6379, client_type='redis', broker_params={}, connection_timeout=300, max_backoff=300, max_retries=0), raise_on_failure=False, progress_engines=6) redis_task_source=MessageBrokerTaskSourceSchema(broker_client=MessageBrokerClientSchema(host='redis', port=6379, client_type='redis', broker_params={}, connection_timeout=300, max_backoff=300, max_retries=0), task_queue='morpheus_task_queue', raise_on_failure=False, progress_engines=6) table_extractor_module=TableExtractorSchema(max_queue_size=1, n_workers=2, raise_on_failure=False, stage_config=None) vdb_task_sink=VdbTaskSinkSchema(recreate=False, service='milvus', is_service_serialized=False, default_resource_name='nv_ingest_collection', resource_schemas={'nv_ingest_collection': {'index_conf': {'field_name': 'vector', 'metric_type': 'L2', 'index_type': 'GPU_CAGRA', 'params': {'intermediate_graph_degree': 128, 'graph_degree': 64, 'build_algo': 'NN_DESCENT'}}, 'schema_conf': {'enable_dynamic_field': True, 'schema_fields': [{'name': 'pk', 'description': 'Primary key for the collection', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'text', 'description': 'Extracted content', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'vector', 'description': 'Embedding vectors', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1024}}, {'name': 'source', 'description': 'Source document and raw data extracted content', 'type': <DataType.JSON: 23>}, {'name': 'content_metadata', 'description': 'Content metadata', 'type': <DataType.JSON: 23>}], 'description': 'NV-INGEST collection schema'}}}, resource_kwargs={}, service_kwargs={}, batch_size=5120, write_time_interval=1.0, retry_interval=60.0, raise_on_failure=False, progress_engines=1)\n",
      "====Pipeline Pre-build====\n",
      "====Pre-Building Segment: main====\n",
      "====Pre-Building Segment Complete!====\n",
      "====Pipeline Pre-build Complete!====\n",
      "====Registering Pipeline====\n",
      "====Building Pipeline====\n",
      "====Building Pipeline Complete!====\n",
      "====Registering Pipeline Complete!====\n",
      "====Starting Pipeline====\n",
      "====Pipeline Started====\n",
      "====Building Segment: main====\n",
      "Added source: <broker_listener-0; LinearModuleSourceStageCPU(module_config=<morpheus.utils.module_utils.ModuleLoader object at 0x7e028e43b3d0>, output_port_name=output, output_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>)>\n",
      "  └─> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <submitted_job_counter-1; LinearModuleStageCPU(module_config=<morpheus.utils.module_utils.ModuleLoader object at 0x7e028e43b490>, input_port_name=input, output_port_name=output, input_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>, output_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <metadata_injection-2; LinearModuleStageCPU(module_config=<morpheus.utils.module_utils.ModuleLoader object at 0x7e028e43b670>, input_port_name=input, output_port_name=output, input_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>, output_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <extractb40373e8521741bfb5f50e52fbc23d8b-3; MultiProcessingBaseStage(task=extract, task_desc=pdf_content_extractor, pe_count=8, process_fn=functools.partial(<function process_pdf_bytes at 0x7e02900be560>, validated_config=PDFExtractorSchema(max_queue_size=1, n_workers=16, raise_on_failure=False, pdfium_config=PDFiumConfigSchema(auth_token='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', yolox_endpoints=('page-elements:8001', 'https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2'), yolox_infer_protocol='http', nim_batch_size=4, workers_per_progress_engine=5), nemoretriever_parse_config=NemoRetrieverParseConfigSchema(auth_token='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', nemoretriever_parse_endpoints=(None, 'http://nemoretriever-parse:8000/v1/chat/completions'), nemoretriever_parse_infer_protocol='http', model_name='nvidia/nemoretriever-parse', timeout=300.0, workers_per_progress_engine=5))), document_type=pdf, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <extractece4cb1d6d454c94b6381f4efdeb2227-4; MultiProcessingBaseStage(task=extract, task_desc=image_content_extractor, pe_count=4, process_fn=functools.partial(<function process_image at 0x7e0290c40310>, validated_config=ImageExtractorSchema(max_queue_size=1, n_workers=16, raise_on_failure=False, image_extraction_config=ImageConfigSchema(auth_token='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', yolox_endpoints=('page-elements:8001', 'https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2'), yolox_infer_protocol='http'))), document_type=regex:^(png|jpeg|jpg|tiff|bmp)$, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <extract0f3d663a8976434cbfeb50786a654747-5; MultiProcessingBaseStage(task=extract, task_desc=docx_content_extractor, pe_count=4, process_fn=functools.partial(<function _process_docx_bytes at 0x7e02917b1c60>, validated_config=DocxExtractorSchema(max_queue_size=1, n_workers=16, raise_on_failure=False, docx_extraction_config=DocxConfigSchema(auth_token='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', yolox_endpoints=('page-elements:8001', 'https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2'), yolox_infer_protocol='http'))), document_type=docx, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <extractb12fc0a720f64d70bbd6916dba9cec4d-6; MultiProcessingBaseStage(task=extract, task_desc=pptx_content_extractor, pe_count=4, process_fn=functools.partial(<function _process_pptx_bytes at 0x7e028ecde4d0>, validated_config=PPTXExtractorSchema(max_queue_size=1, n_workers=16, raise_on_failure=False, pptx_extraction_config=PPTXConfigSchema(auth_token='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', yolox_endpoints=('page-elements:8001', 'https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2'), yolox_infer_protocol='http'))), document_type=pptx, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <audio_data_extract292aa8a541b94ffbbe1ea537b70fc8e6-7; MultiProcessingBaseStage(task=audio_data_extract, task_desc=audio_data_extraction, pe_count=4, process_fn=functools.partial(<function _transcribe_audio at 0x7e0290b66290>, validated_config=AudioExtractorSchema(max_queue_size=1, n_workers=16, raise_on_failure=False, audio_extraction_config=AudioConfigSchema(auth_token='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', audio_endpoints=('audio:50051', None), audio_infer_protocol='grpc', function_id='', use_ssl=None, ssl_cert=None))), document_type=None, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <dedup4ca91fb240404569bedf02b8d0d03712-8; MultiProcessingBaseStage(task=dedup, task_desc=dedup_images, pe_count=4, process_fn=functools.partial(<function dedup_image_stage at 0x7e0290c408b0>, validated_config=ImageDedupSchema(raise_on_failure=False, cpu_only=False)), document_type=None, filter_properties={'content_type': 'image'})>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <filter94f13e5580ce45219ba13d3f8d9d0b1e-9; MultiProcessingBaseStage(task=filter, task_desc=filter_images, pe_count=4, process_fn=functools.partial(<function image_filter_stage at 0x7e0290c40ca0>, validated_config=ImageFilterSchema(raise_on_failure=False, cpu_only=False)), document_type=None, filter_properties={'content_type': 'image'})>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <table_data_extractbb759ff3c3d04e759f4cf50de649d1eb-10; MultiProcessingBaseStage(task=table_data_extract, task_desc=table_data_extraction, pe_count=4, process_fn=functools.partial(<function _extract_table_data at 0x7e02900be320>, validated_config=TableExtractorSchema(max_queue_size=1, n_workers=2, raise_on_failure=False, stage_config=TableExtractorConfigSchema(auth_token='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', yolox_endpoints=('table-structure:8001', 'https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1'), yolox_infer_protocol='http', paddle_endpoints=('paddle:8001', 'https://ai.api.nvidia.com/v1/cv/baidu/paddleocr'), paddle_infer_protocol='http', nim_batch_size=2, workers_per_progress_engine=5))), document_type=None, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <chart_data_extract8f1c51f73e3c40d6a14ccbce69371330-11; MultiProcessingBaseStage(task=chart_data_extract, task_desc=chart_data_extraction, pe_count=4, process_fn=functools.partial(<function _extract_chart_data at 0x7e02900bdd80>, validated_config=ChartExtractorSchema(max_queue_size=1, n_workers=2, raise_on_failure=False, stage_config=ChartExtractorConfigSchema(auth_token='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', yolox_endpoints=('graphic-elements:8001', 'https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1'), yolox_infer_protocol='http', paddle_endpoints=('paddle:8001', 'https://ai.api.nvidia.com/v1/cv/baidu/paddleocr'), paddle_infer_protocol='http', nim_batch_size=2, workers_per_progress_engine=5))), document_type=None, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <infographic_data_extract091805bd485140f78fce4927357f0e20-12; MultiProcessingBaseStage(task=infographic_data_extract, task_desc=infographic_data_extraction, pe_count=4, process_fn=functools.partial(<function _extract_infographic_data at 0x7e02900be050>, validated_config=InfographicExtractorSchema(max_queue_size=1, n_workers=2, raise_on_failure=False, stage_config=InfographicExtractorConfigSchema(auth_token='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', paddle_endpoints=('paddle:8001', 'https://ai.api.nvidia.com/v1/cv/baidu/paddleocr'), paddle_infer_protocol='http', nim_batch_size=2, workers_per_progress_engine=5))), document_type=None, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <text_splitter-14; LinearModuleStageCPU(module_config=<morpheus.utils.module_utils.ModuleLoader object at 0x7e028e49fca0>, input_port_name=input, output_port_name=output, input_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>, output_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <caption06754d8878f54ee4bed12e7fa0acbcea-13; MultiProcessingBaseStage(task=caption, task_desc=caption_ext, pe_count=4, process_fn=functools.partial(<function caption_extract_stage at 0x7e028e450550>, validated_config=ImageCaptionExtractionSchema(api_key='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', endpoint_url='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', prompt='Caption the content of this image:', model_name='meta/llama-3.2-11b-vision-instruct', raise_on_failure=False)), document_type=None, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <embed320f0a609b9c44829d569cf0e1886bd3-15; MultiProcessingBaseStage(task=embed, task_desc=embed_text, pe_count=4, process_fn=functools.partial(<function _generate_text_embeddings_df at 0x7e0290c40040>, validated_config=EmbedExtractionsSchema(api_key='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', batch_size=8192, embedding_model='nvidia/llama-3.2-nv-embedqa-1b-v2', embedding_nim_endpoint='https://integrate.api.nvidia.com/v1', encoding_format='float', httpx_log_level=<LogLevel.WARNING: 'WARNING'>, input_type='passage', raise_on_failure=False, truncate='END')), document_type=None, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <image-storage-17; ImageStorageStage(module_config=None, raise_on_failure=False)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <store_embedding13161e2ab3cb437fae37ed0d5d4b0256-16; MultiProcessingBaseStage(task=store_embedding, task_desc=store_embedding_minio, pe_count=4, process_fn=functools.partial(<function _store_embeddings at 0x7e028e8b5870>, validated_config=EmbeddingStorageModuleSchema(raise_on_failure=False)), document_type=None, filter_properties=None)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <broker_task_sink-18; LinearModuleStageCPU(module_config=<morpheus.utils.module_utils.ModuleLoader object at 0x7e028e49c5b0>, input_port_name=input, output_port_name=output, input_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>, output_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <otel_meter-20; LinearModuleStageCPU(module_config=<morpheus.utils.module_utils.ModuleLoader object at 0x7e028e49c2b0>, input_port_name=input, output_port_name=output, input_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>, output_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <otel_tracer-19; LinearModuleStageCPU(module_config=<morpheus.utils.module_utils.ModuleLoader object at 0x7e028e49c430>, input_port_name=input, output_port_name=output, input_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>, output_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "Added stage: <completed_job_counter-21; LinearModuleStageCPU(module_config=<morpheus.utils.module_utils.ModuleLoader object at 0x7e028e49c130>, input_port_name=input, output_port_name=output, input_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>, output_type=<class 'nv_ingest_api.primitives.ingest_control_message.IngestControlMessage'>)>\n",
      "  └─ nv_ingest_api.IngestControlMessage -> nv_ingest_api.IngestControlMessage\n",
      "====Building Segment Complete!====\n",
      "INFO:     172.18.0.7:50158 - \"GET /v1/health/ready HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker logs compose-nv-ingest-ms-runtime-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c235f-9588-4623-aea0-030fc2e71ddc",
   "metadata": {},
   "source": [
    "### Deploying The RAG Server\n",
    "\n",
    "The RAG server in the NVIDIA RAG Blueprint is a core microservice that orchestrates the entire Retrieval-Augmented Generation (RAG) pipeline. It handles incoming user queries, coordinates retrieval of relevant information from the vector database, and manages the interaction with large language models to generate context-aware responses. \n",
    "\n",
    "The RAG server is based on LangChain, exposes APIs for user interaction, and integrates with other components like retrievers, rerankers, and document ingestion services to deliver accurate, enterprise-ready generative AI solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d9fd2-2502-45ae-a922-f25c985ed9e7",
   "metadata": {},
   "source": [
    "#### Pull The Containers Needed The RAG Server Microservice\n",
    "\n",
    "As a prerequisite, we'll need to pull the containers associated with the RAG pipeline before deploying the containers. The code block below will facilitate this process. Once the relevant images are pulled, we can move on to configuring parameters that will be used to customize deployment of the RAG services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6593cb9-a749-49fa-9bae-b1107e1a6644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " rag-playground Pulling \n",
      " rag-server Pulling \n",
      " rag-server Pulled \n",
      " rag-playground Pulled \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker compose -f ../repos/nvidia_rag/deploy/compose/docker-compose-rag-server.yaml pull --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e09e02-ec99-4f4d-b2e0-9b3f85a19b1c",
   "metadata": {},
   "source": [
    "#### Configure Parameters For The RAG Server Microservice\n",
    "\n",
    "By default, the RAG Server Microservice, expects to leverage locally hosted NVIDIA NIMs. To simplify this playbook and to ensure users are able to run Tokkio + RAG on the same instance, we'll leverage models hosted on NVIDIA Foundation Endpoints through [build.nvidia.com.](https://build.nvidia.com/)\n",
    "\n",
    "We'll configure the service to use the default LLM & Retriever NIMs enabled in the default RAG blueprint. We'll also set the corresponding Server URL's to empty strings, as for the given containers, an empty server URL string will default to the models hosted on build. Please note that changing the model names will effectively change which models are invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df56fdd4-0501-46f3-9386-587a8188543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"APP_LLM_MODELNAME\"] = \"meta/llama-3.1-70b-instruct\"\n",
    "os.environ[\"APP_LLM_SERVERURL\"] = \"\"\n",
    "os.environ[\"APP_EMBEDDINGS_MODELNAME\"] = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "os.environ[\"APP_EMBEDDINGS_SERVERURL\"] = \"\"\n",
    "os.environ[\"APP_RANKING_MODELNAME\"] = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n",
    "os.environ[\"APP_RANKING_SERVERURL\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c60f22-4d09-4561-bba8-17080b55fd1b",
   "metadata": {},
   "source": [
    "#### Start The Containers For The RAG Server Microservice\n",
    "\n",
    "With the parameters for this deployment configured, we can start the service using the `docker compose` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4b84595-9d42-41b6-abb3-83743e3c43cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=\"2025-05-07T19:34:22Z\" level=warning msg=\"Found orphan containers ([compose-nv-ingest-ms-runtime-1 ingestor-server compose-redis-1 milvus-standalone milvus-minio milvus-etcd]) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\"\n",
      " Container rag-server  Creating\n",
      " Container rag-server  Created\n",
      " Container rag-playground  Creating\n",
      " Container rag-playground  Created\n",
      " Container rag-server  Starting\n",
      " Container rag-server  Started\n",
      " Container rag-playground  Starting\n",
      " Container rag-playground  Started\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker compose -f ../repos/nvidia_rag/deploy/compose/docker-compose-rag-server.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d29b2e-3240-4024-8fc2-078de03599ff",
   "metadata": {},
   "source": [
    "#### Verify Status Of RAG Server Containers\n",
    "\n",
    "Once the containers have been started, we can verify the status below. Each service should either have a `healthy` status or a non-error status next to the appropriate service name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c948b063-9f41-43e3-98c3-e6475f3d5512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   NAMES                            STATUS\n",
      "6cf111e897bf   rag-playground                   Up 3 seconds\n",
      "92a651fdec80   rag-server                       Up 3 seconds\n",
      "ac450966da38   compose-nv-ingest-ms-runtime-1   Up 46 seconds (healthy)\n",
      "ae83680b62d2   ingestor-server                  Up 46 seconds\n",
      "7dd3f3e440f7   compose-redis-1                  Up 46 seconds\n",
      "3e1ac2226efd   milvus-standalone                Up 58 seconds\n",
      "1cc93d91a4f3   milvus-minio                     Up 58 seconds (healthy)\n",
      "9f1091c4de6a   milvus-etcd                      Up 58 seconds (healthy)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker ps --format \"table {{.ID}}\\t{{.Names}}\\t{{.Status}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c222e7-3c2b-4608-b1cd-12ff7f885e8e",
   "metadata": {},
   "source": [
    "#### Verify Application Startup Is Successful\n",
    "\n",
    "It's helpful to verify if the application started up as expected, even with the presence of the `healthy` status on the `rag-server` or `rag-playground` services. We can view the status of each service by checking the logs. We'll start for the `rag-server` service using the command below, if we see the following output towards the end without any error logs, the RAG Server is operating as expected:\n",
    "\n",
    "```\n",
    "INFO:src.server:Initializing NVIDIA RAG server...\n",
    "INFO:     Started server process [15]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9103eaa6-99c9-404c-9645-edd32c9ff014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Uvicorn running on http://0.0.0.0:8081 (Press CTRL+C to quit)\n",
      "INFO:     Started parent process [1]\n",
      "Optional nv_ingest_client module not installed.\n",
      "Optional nv_ingest_client module not installed.\n",
      "Optional nv_ingest_client module not installed.\n",
      "Optional nv_ingest_client module not installed.\n",
      "Optional nv_ingest_client module not installed.\n",
      "Optional nv_ingest_client module not installed.\n",
      "Optional nv_ingest_client module not installed.\n",
      "Optional nv_ingest_client module not installed.\n",
      "Collection 'multimodal_data' does not exist in Milvus. Aborting vectorstore creation.\n",
      "Collection 'multimodal_data' does not exist in Milvus. Aborting vectorstore creation.\n",
      "Collection 'multimodal_data' does not exist in Milvus. Aborting vectorstore creation.\n",
      "Collection 'multimodal_data' does not exist in Milvus. Aborting vectorstore creation.\n",
      "Collection 'multimodal_data' does not exist in Milvus. Aborting vectorstore creation.\n",
      "Collection 'multimodal_data' does not exist in Milvus. Aborting vectorstore creation.\n",
      "Collection 'multimodal_data' does not exist in Milvus. Aborting vectorstore creation.\n",
      "Collection 'multimodal_data' does not exist in Milvus. Aborting vectorstore creation.\n",
      "INFO:src.server:Initializing NVIDIA RAG server...\n",
      "INFO:src.server:Initializing NVIDIA RAG server...\n",
      "INFO:src.server:Initializing NVIDIA RAG server...\n",
      "INFO:src.server:Initializing NVIDIA RAG server...\n",
      "INFO:     Started server process [10]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [11]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:src.server:Initializing NVIDIA RAG server...\n",
      "INFO:src.server:Initializing NVIDIA RAG server...\n",
      "INFO:src.server:Initializing NVIDIA RAG server...\n",
      "INFO:     Started server process [8]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:src.server:Initializing NVIDIA RAG server...\n",
      "INFO:     Started server process [15]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [14]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [13]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [9]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Started server process [12]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker logs rag-server "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be49d6-be1e-4368-af9b-23058871967d",
   "metadata": {},
   "source": [
    "Similarly, if we observe the output below for the `rag-playground` service, the application startup was successful:\n",
    "\n",
    "```\n",
    "✓ Starting...\n",
    "✓ Ready in 458ms\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54fda8d9-3572-495b-bf7c-0f854006100e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> rag-2@0.1.0 start\n",
      "> next start\n",
      "\n",
      "   ▲ Next.js 15.1.6\n",
      "   - Local:        http://localhost:3000\n",
      "   - Network:      http://172.18.0.9:3000\n",
      "\n",
      " ✓ Starting...\n",
      " ✓ Ready in 437ms\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker logs rag-playground "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f515b3-02a9-423b-ac8f-43fa38024ab9",
   "metadata": {},
   "source": [
    "## Accessing The RAG Playground\n",
    "\n",
    "The RAG playground can be accessed on the endpoint `http://<application-instance-ip>:8090`. Once this port has been exposed, you should be able to see the following:\n",
    "\n",
    "![ace_rag-playground](../images/rag-playground.png)\n",
    "\n",
    "Once you have access to the UI, you can create a Milvus collection - think of this as a unique collection where documents under a specific subject matter are stored. We have multiple collections that can be created, but as a start created a collection called `dht_tokkio_rag`. You'll also be prompted upload a document. The document can be a `.txt` document or even `.pdf` or `.docx` file since we're leveraging the multimodal ingestion pipeline (NV-Ingest). Please refer to the [NV-Ingest documentation](https://docs.nvidia.com/nemo/retriever/extraction/overview/#what-nemo-retriever-extraction-is) page for the full list of supported file types.\n",
    "\n",
    "You can interact with service without specifying a collection - this will use the base models foundational knowledge to generate a response. To trigger a conversation using RAG, simply provide the collection you ingested your documents in. You can always create another collection or add to a preexisting collection using the `Add Source` option! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101e25d-b8bb-443d-82cd-9baea48decb1",
   "metadata": {},
   "source": [
    "## Accessing The RAG Server\n",
    "\n",
    "The Swagger UI for the RAG server can be accessed on the endpoint http://<application-instance-ip>:8081/docs. Once this port has been exposed, you should be able to see the following:\n",
    "\n",
    "![ace_rag-server](../images/rag-server.png)\n",
    "\n",
    "The Swagger UI is a great tool for understanding the RAG Server API schema ansd can be used to test and submit requests to the pipleine. The `/chat/completions` endpoint is particularly useful, as we'll be leveraging this endpoint to connect our Tokkio Avatar to our RAG service via the ACE configurator in the `customizing_tokkio.ipynb` notebook. For example, here's a default request made to the endpoint:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Hello! What can you help me with?\"\n",
    "    }\n",
    "  ],\n",
    "  \"use_knowledge_base\": true,\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"reranker_top_k\": 10,\n",
    "  \"vdb_top_k\": 100,\n",
    "  \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "  \"collection_name\": \"multimodal_data\",\n",
    "  \"enable_query_rewriting\": false,\n",
    "  \"enable_reranker\": true,\n",
    "  \"enable_guardrails\": false,\n",
    "  \"enable_citations\": true,\n",
    "  \"model\": \"meta/llama-3.1-70b-instruct\",\n",
    "  \"llm_endpoint\": \"\",\n",
    "  \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "  \"embedding_endpoint\": \"\",\n",
    "  \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "  \"reranker_endpoint\": \"\",\n",
    "  \"stop\": []\n",
    "}\n",
    "```\n",
    "\n",
    "Take note of the parameters used here, as we'll leverage this same schema to connect to our RAG server!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5037d-0404-4f93-bc31-d4ecf5ed68a0",
   "metadata": {},
   "source": [
    "## Cleaing Up Services (Optional)\n",
    "\n",
    "If you want to clean up all the services we deployed earlier, the code block below can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "99b6a8bf-09a5-4cb4-9e14-d4c992d2de94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container milvus-standalone  Stopping\n",
      " Container milvus-standalone  Stopped\n",
      " Container milvus-standalone  Removing\n",
      " Container milvus-standalone  Removed\n",
      " Container milvus-minio  Stopping\n",
      " Container milvus-etcd  Stopping\n",
      " Container milvus-etcd  Stopped\n",
      " Container milvus-etcd  Removing\n",
      " Container milvus-etcd  Removed\n",
      " Container milvus-minio  Stopped\n",
      " Container milvus-minio  Removing\n",
      " Container milvus-minio  Removed\n",
      " Network nvidia-rag  Removing\n",
      " Network nvidia-rag  Resource is still in use\n",
      " Container compose-nv-ingest-ms-runtime-1  Stopping\n",
      " Container compose-redis-1  Stopping\n",
      " Container ingestor-server  Stopping\n",
      " Container compose-redis-1  Stopped\n",
      " Container compose-redis-1  Removing\n",
      " Container compose-redis-1  Removed\n",
      " Container compose-nv-ingest-ms-runtime-1  Stopped\n",
      " Container compose-nv-ingest-ms-runtime-1  Removing\n",
      " Container compose-nv-ingest-ms-runtime-1  Removed\n",
      " Container ingestor-server  Stopped\n",
      " Container ingestor-server  Removing\n",
      " Container ingestor-server  Removed\n",
      " Network nvidia-rag  Removing\n",
      " Network nvidia-rag  Resource is still in use\n",
      " Container rag-playground  Stopping\n",
      " Container rag-playground  Stopped\n",
      " Container rag-playground  Removing\n",
      " Container rag-playground  Removed\n",
      " Container rag-server  Stopping\n",
      " Container rag-server  Stopped\n",
      " Container rag-server  Removing\n",
      " Container rag-server  Removed\n",
      " Network nvidia-rag  Removing\n",
      " Network nvidia-rag  Removed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# clean up milvus service\n",
    "docker compose -f ../repos/nvidia_rag/deploy/compose/vectordb.yaml down\n",
    "# clean up ingestor service\n",
    "docker compose -f ../repos/nvidia_rag/deploy/compose/docker-compose-ingestor-server.yaml down\n",
    "# clean up rag service\n",
    "docker compose -f ../repos/nvidia_rag/deploy/compose/docker-compose-rag-server.yaml down"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
